{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a8275e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score, ParameterGrid\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy import sparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b085ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507584df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f61e7",
   "metadata": {},
   "source": [
    "# Clean the data and do data engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82f493c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TSFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Leakage-safe feature engineering for panel time-series-in-a-row.\n",
    "\n",
    "    Guarantees:\n",
    "    - Cross-sectional / group-relative features use ONLY training fold data via fit().\n",
    "    - All within-TS computations are within the same TS snapshot.\n",
    "    - TS/ALLOCATION are not emitted as model features (avoids identity leakage).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 ts_col=\"TS\",\n",
    "                 group_col=\"GROUP\",\n",
    "                 alloc_col=\"ALLOCATION\",\n",
    "                 turnover_col=\"MEDIAN_DAILY_TURNOVER\",\n",
    "                 ret_prefix=\"RET_\",\n",
    "                 sv_prefix=\"SIGNED_VOLUME_\",\n",
    "                 horizons=(3, 5, 10, 20)):\n",
    "        # IMPORTANT: do NOT modify parameters here (sklearn clone requirement)\n",
    "        self.ts_col = ts_col\n",
    "        self.group_col = group_col\n",
    "        self.alloc_col = alloc_col\n",
    "        self.turnover_col = turnover_col\n",
    "        self.ret_prefix = ret_prefix\n",
    "        self.sv_prefix = sv_prefix\n",
    "        self.horizons = horizons  # store exactly as provided\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    @staticmethod\n",
    "    def _get_lag_cols(df, prefix):\n",
    "        cols = [c for c in df.columns if c.startswith(prefix)]\n",
    "\n",
    "        def _idx(c):\n",
    "            try:\n",
    "                return int(c.split(\"_\")[-1])\n",
    "            except Exception:\n",
    "                return 10**9\n",
    "\n",
    "        return sorted(cols, key=_idx)\n",
    "\n",
    "    @staticmethod\n",
    "    def _row_sign_flips(arr_1d):\n",
    "        x = np.asarray(arr_1d, dtype=float)\n",
    "        x = x[~np.isnan(x)]\n",
    "        if x.size <= 1:\n",
    "            return 0\n",
    "        s = np.sign(x)\n",
    "\n",
    "        # carry-forward for zeros\n",
    "        for i in range(1, len(s)):\n",
    "            if s[i] == 0:\n",
    "                s[i] = s[i - 1]\n",
    "        if len(s) > 0 and s[0] == 0:\n",
    "            nz = s[s != 0]\n",
    "            if nz.size > 0:\n",
    "                s[0] = nz[0]\n",
    "        s = s[s != 0]\n",
    "        if s.size <= 1:\n",
    "            return 0\n",
    "        return int(np.sum(s[1:] != s[:-1]))\n",
    "\n",
    "    @staticmethod\n",
    "    def _row_longest_streak(arr_1d, sign=1):\n",
    "        x = np.asarray(arr_1d, dtype=float)\n",
    "        x = x[~np.isnan(x)]\n",
    "        if x.size == 0:\n",
    "            return 0\n",
    "        s = np.sign(x)\n",
    "        target = 1 if sign > 0 else -1\n",
    "        best = cur = 0\n",
    "        for v in s:\n",
    "            if v == target:\n",
    "                cur += 1\n",
    "                best = max(best, cur)\n",
    "            else:\n",
    "                cur = 0\n",
    "        return int(best)\n",
    "\n",
    "    @staticmethod\n",
    "    def _row_slope(arr_1d):\n",
    "        x = np.asarray(arr_1d, dtype=float)\n",
    "        mask = ~np.isnan(x)\n",
    "        if mask.sum() < 2:\n",
    "            return np.nan\n",
    "        y = x[mask]\n",
    "        t = np.arange(1, y.size + 1, dtype=float)\n",
    "        t = t - t.mean()\n",
    "        y = y - y.mean()\n",
    "        denom = np.sum(t * t)\n",
    "        if denom == 0:\n",
    "            return 0.0\n",
    "        return float(np.sum(t * y) / denom)\n",
    "\n",
    "    @staticmethod\n",
    "    def _row_corr(a_1d, b_1d):\n",
    "        a = np.asarray(a_1d, dtype=float)\n",
    "        b = np.asarray(b_1d, dtype=float)\n",
    "        m = ~np.isnan(a) & ~np.isnan(b)\n",
    "        if m.sum() < 3:\n",
    "            return np.nan\n",
    "        aa = a[m]\n",
    "        bb = b[m]\n",
    "        sa = np.std(aa)\n",
    "        sb = np.std(bb)\n",
    "        if sa == 0 or sb == 0:\n",
    "            return 0.0\n",
    "        return float(np.corrcoef(aa, bb)[0, 1])\n",
    "\n",
    "    @staticmethod\n",
    "    def _bucketize(x, q1, q2, labels=(\"low\", \"mid\", \"high\")):\n",
    "        out = np.full(x.shape, None, dtype=object)\n",
    "        m = ~np.isnan(x)\n",
    "        out[m & (x <= q1)] = labels[0]\n",
    "        out[m & (x > q1) & (x <= q2)] = labels[1]\n",
    "        out[m & (x > q2)] = labels[2]\n",
    "        return out\n",
    "\n",
    "    # ---------- fit/transform ----------\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "\n",
    "        # normalize horizons here (OK to modify internal fitted attrs)\n",
    "        self.horizons_ = tuple(sorted(set(self.horizons)))\n",
    "\n",
    "        self.ret_cols_ = self._get_lag_cols(X, self.ret_prefix)\n",
    "        self.sv_cols_  = self._get_lag_cols(X, self.sv_prefix)\n",
    "\n",
    "        self.has_ts_ = self.ts_col in X.columns\n",
    "        self.has_group_ = self.group_col in X.columns\n",
    "        self.has_turnover_ = self.turnover_col in X.columns\n",
    "\n",
    "        feats = self._build_row_features(X)\n",
    "\n",
    "        # Learn volatility bucket thresholds on train fold\n",
    "        vol = feats[\"ret_std_20\"].to_numpy(dtype=float)\n",
    "        vol_clean = vol[~np.isnan(vol)]\n",
    "        if vol_clean.size >= 10:\n",
    "            self.vol_q1_, self.vol_q2_ = np.quantile(vol_clean, [0.33, 0.66])\n",
    "        else:\n",
    "            self.vol_q1_, self.vol_q2_ = (np.nan, np.nan)\n",
    "\n",
    "        # Learn turnover bucket thresholds on train fold\n",
    "        if self.has_turnover_:\n",
    "            to = feats[\"turnover\"].to_numpy(dtype=float)\n",
    "            to_clean = to[~np.isnan(to)]\n",
    "            if to_clean.size >= 10:\n",
    "                self.to_q1_, self.to_q2_ = np.quantile(to_clean, [0.33, 0.66])\n",
    "            else:\n",
    "                self.to_q1_, self.to_q2_ = (np.nan, np.nan)\n",
    "        else:\n",
    "            self.to_q1_, self.to_q2_ = (np.nan, np.nan)\n",
    "\n",
    "        # Learn within-TS / within-(TS,GROUP) medians using ONLY train fold\n",
    "        if self.has_ts_:\n",
    "            feats[\"_TS_KEY\"] = X[self.ts_col].astype(str)\n",
    "\n",
    "            base_cols = [\"ret_mean_5\", \"ret_mean_20\", \"sv_sum_5\", \"sv_sum_20\", \"ret_pos_frac_20\"]\n",
    "            self.ts_medians_ = feats.groupby(\"_TS_KEY\", observed=True)[base_cols].median()\n",
    "\n",
    "            if self.has_group_:\n",
    "                feats[\"_GRP_KEY\"] = X[self.group_col].astype(str)\n",
    "                self.ts_grp_medians_ = feats.groupby([\"_TS_KEY\", \"_GRP_KEY\"], observed=True)[base_cols].median()\n",
    "            else:\n",
    "                self.ts_grp_medians_ = None\n",
    "        else:\n",
    "            self.ts_medians_ = None\n",
    "            self.ts_grp_medians_ = None\n",
    "\n",
    "        # global fallback medians from train fold\n",
    "        self.global_medians_ = feats[[\"ret_mean_5\", \"ret_mean_20\", \"sv_sum_5\", \"sv_sum_20\", \"ret_pos_frac_20\"]].median(numeric_only=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        feats = self._build_row_features(X)\n",
    "\n",
    "        # Buckets using train-fold thresholds\n",
    "        vol = feats[\"ret_std_20\"].to_numpy(dtype=float)\n",
    "        if np.isnan(self.vol_q1_) or np.isnan(self.vol_q2_):\n",
    "            feats[\"vol_bucket\"] = np.where(np.isnan(vol), None, \"mid\")\n",
    "        else:\n",
    "            feats[\"vol_bucket\"] = self._bucketize(vol, self.vol_q1_, self.vol_q2_)\n",
    "\n",
    "        to = feats[\"turnover\"].to_numpy(dtype=float)\n",
    "        if np.isnan(self.to_q1_) or np.isnan(self.to_q2_):\n",
    "            feats[\"turnover_bucket\"] = np.where(np.isnan(to), None, \"mid\")\n",
    "        else:\n",
    "            feats[\"turnover_bucket\"] = self._bucketize(to, self.to_q1_, self.to_q2_)\n",
    "\n",
    "        # TS-relative and (TS,GROUP)-relative features using ONLY mappings from fit()\n",
    "        if self.has_ts_ and self.ts_medians_ is not None:\n",
    "            ts_key = X[self.ts_col].astype(str)\n",
    "\n",
    "            ts_med = self.ts_medians_.reindex(ts_key).reset_index(drop=True)\n",
    "            cols = [\"ret_mean_5\", \"ret_mean_20\", \"sv_sum_5\", \"sv_sum_20\", \"ret_pos_frac_20\"]\n",
    "            for col in cols:\n",
    "                base = ts_med[col].to_numpy(dtype=float)\n",
    "                m = np.isnan(base)\n",
    "                if m.any():\n",
    "                    base[m] = float(self.global_medians_[col])\n",
    "                feats[f\"{col}_minus_ts_median\"] = feats[col].to_numpy(dtype=float) - base\n",
    "\n",
    "            if self.has_group_ and self.ts_grp_medians_ is not None:\n",
    "                grp_key = X[self.group_col].astype(str)\n",
    "                idx = pd.MultiIndex.from_arrays([ts_key, grp_key])\n",
    "\n",
    "                ts_grp_med = self.ts_grp_medians_.reindex(idx).reset_index(drop=True)\n",
    "                for col in cols:\n",
    "                    base = ts_grp_med[col].to_numpy(dtype=float)\n",
    "                    m = np.isnan(base)\n",
    "                    if m.any():\n",
    "                        base[m] = float(self.global_medians_[col])\n",
    "                    feats[f\"{col}_minus_ts_group_median\"] = feats[col].to_numpy(dtype=float) - base\n",
    "\n",
    "        # Keep GROUP as categorical (optional but useful)\n",
    "        if self.has_group_ and self.group_col in X.columns:\n",
    "            feats[\"GROUP\"] = X[self.group_col].astype(str)\n",
    "\n",
    "        # Do NOT emit TS or ALLOCATION\n",
    "        return feats\n",
    "\n",
    "    def _build_row_features(self, X):\n",
    "        feats = pd.DataFrame(index=X.index)\n",
    "\n",
    "        ret_cols = [c for c in getattr(self, \"ret_cols_\", []) if c in X.columns]\n",
    "        if not ret_cols:\n",
    "            ret_cols = self._get_lag_cols(X, self.ret_prefix)\n",
    "        R = X[ret_cols].to_numpy(dtype=float) if ret_cols else np.empty((len(X), 0))\n",
    "\n",
    "        sv_cols = [c for c in getattr(self, \"sv_cols_\", []) if c in X.columns]\n",
    "        if not sv_cols:\n",
    "            sv_cols = self._get_lag_cols(X, self.sv_prefix)\n",
    "        V = X[sv_cols].to_numpy(dtype=float) if sv_cols else np.empty((len(X), 0))\n",
    "\n",
    "        # 1) Directional stability (returns)\n",
    "        if R.shape[1] > 0:\n",
    "            feats[\"ret_pos_frac_20\"] = np.nanmean((R > 0).astype(float), axis=1)\n",
    "            feats[\"ret_neg_frac_20\"] = np.nanmean((R < 0).astype(float), axis=1)\n",
    "            feats[\"ret_flip_count_20\"] = [self._row_sign_flips(row) for row in R]\n",
    "            feats[\"ret_longest_pos_streak_20\"] = [self._row_longest_streak(row, sign=1) for row in R]\n",
    "            feats[\"ret_longest_neg_streak_20\"] = [self._row_longest_streak(row, sign=-1) for row in R]\n",
    "        else:\n",
    "            feats[\"ret_pos_frac_20\"] = np.nan\n",
    "            feats[\"ret_neg_frac_20\"] = np.nan\n",
    "            feats[\"ret_flip_count_20\"] = 0\n",
    "            feats[\"ret_longest_pos_streak_20\"] = 0\n",
    "            feats[\"ret_longest_neg_streak_20\"] = 0\n",
    "\n",
    "        # 2) Window aggregates (returns)\n",
    "        for h in self.horizons_ if hasattr(self, \"horizons_\") else tuple(sorted(set(self.horizons))):\n",
    "            if R.shape[1] >= h:\n",
    "                Rh = R[:, :h]\n",
    "                feats[f\"ret_mean_{h}\"] = np.nanmean(Rh, axis=1)\n",
    "                feats[f\"ret_median_{h}\"] = np.nanmedian(Rh, axis=1)\n",
    "                feats[f\"ret_std_{h}\"] = np.nanstd(Rh, axis=1)\n",
    "                feats[f\"ret_min_{h}\"] = np.nanmin(Rh, axis=1)\n",
    "                feats[f\"ret_max_{h}\"] = np.nanmax(Rh, axis=1)\n",
    "                feats[f\"ret_slope_{h}\"] = [self._row_slope(row) for row in Rh]\n",
    "            else:\n",
    "                feats[f\"ret_mean_{h}\"] = np.nan\n",
    "                feats[f\"ret_median_{h}\"] = np.nan\n",
    "                feats[f\"ret_std_{h}\"] = np.nan\n",
    "                feats[f\"ret_min_{h}\"] = np.nan\n",
    "                feats[f\"ret_max_{h}\"] = np.nan\n",
    "                feats[f\"ret_slope_{h}\"] = np.nan\n",
    "\n",
    "        # 3) Vol regime helpers\n",
    "        feats[\"ret_mean5_minus_mean20\"] = feats.get(\"ret_mean_5\", np.nan) - feats.get(\"ret_mean_20\", np.nan)\n",
    "        feats[\"ret_std5_over_std20\"] = feats.get(\"ret_std_5\", np.nan) / (feats.get(\"ret_std_20\", np.nan) + 1e-12)\n",
    "        feats[\"vol_expansion_flag\"] = (feats[\"ret_std5_over_std20\"] > 1.0).astype(float)\n",
    "\n",
    "        # 4) Signed volume consistency + disagreement\n",
    "        if V.shape[1] > 0:\n",
    "            feats[\"sv_pos_frac_20\"] = np.nanmean((V > 0).astype(float), axis=1)\n",
    "            feats[\"sv_neg_frac_20\"] = np.nanmean((V < 0).astype(float), axis=1)\n",
    "            feats[\"sv_flip_count_20\"] = [self._row_sign_flips(row) for row in V]\n",
    "        else:\n",
    "            feats[\"sv_pos_frac_20\"] = np.nan\n",
    "            feats[\"sv_neg_frac_20\"] = np.nan\n",
    "            feats[\"sv_flip_count_20\"] = 0\n",
    "\n",
    "        for h in self.horizons_ if hasattr(self, \"horizons_\") else tuple(sorted(set(self.horizons))):\n",
    "            if V.shape[1] >= h:\n",
    "                Vh = V[:, :h]\n",
    "                feats[f\"sv_sum_{h}\"] = np.nansum(Vh, axis=1)\n",
    "                feats[f\"sv_mean_{h}\"] = np.nanmean(Vh, axis=1)\n",
    "                feats[f\"sv_std_{h}\"] = np.nanstd(Vh, axis=1)\n",
    "                feats[f\"sv_slope_{h}\"] = [self._row_slope(row) for row in Vh]\n",
    "            else:\n",
    "                feats[f\"sv_sum_{h}\"] = np.nan\n",
    "                feats[f\"sv_mean_{h}\"] = np.nan\n",
    "                feats[f\"sv_std_{h}\"] = np.nan\n",
    "                feats[f\"sv_slope_{h}\"] = np.nan\n",
    "\n",
    "        if R.shape[1] > 0 and V.shape[1] > 0:\n",
    "            h = min(R.shape[1], V.shape[1], 20)\n",
    "            feats[\"flow_return_corr_20\"] = [self._row_corr(r, v) for r, v in zip(R[:, :h], V[:, :h])]\n",
    "        else:\n",
    "            feats[\"flow_return_corr_20\"] = np.nan\n",
    "\n",
    "        ret_dir_5 = np.sign(feats.get(\"ret_mean_5\", np.nan).to_numpy(dtype=float))\n",
    "        sv_dir_5 = np.sign(feats.get(\"sv_sum_5\", np.nan).to_numpy(dtype=float))\n",
    "        feats[\"flow_vs_return_disagree_5\"] = ((ret_dir_5 * sv_dir_5) < 0).astype(float)\n",
    "\n",
    "        # 5) Turnover regime (keep NaN + missing flag)\n",
    "        if self.turnover_col in X.columns:\n",
    "            to = X[self.turnover_col].to_numpy(dtype=float)\n",
    "            feats[\"turnover\"] = to\n",
    "            feats[\"turnover_missing\"] = np.isnan(to).astype(float)\n",
    "            feats[\"log_turnover\"] = np.log1p(np.where(np.isnan(to), 0.0, np.abs(to)))\n",
    "        else:\n",
    "            feats[\"turnover\"] = np.nan\n",
    "            feats[\"turnover_missing\"] = 1.0\n",
    "            feats[\"log_turnover\"] = np.nan\n",
    "\n",
    "        scale = feats[\"log_turnover\"].to_numpy(dtype=float)\n",
    "        denom = np.where(np.isnan(scale) | (scale <= 0), np.nan, scale)\n",
    "        feats[\"sv_sum_20_over_log_turnover\"] = feats.get(\"sv_sum_20\", np.nan) / (denom + 1e-12)\n",
    "\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ee61fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Robust data prep (ROW_ID-safe)\n",
    "# -------------------------\n",
    "X_train = pd.read_csv(\"data/X_train.csv\", index_col=\"ROW_ID\")\n",
    "X_test  = pd.read_csv(\"data/X_test.csv\",  index_col=\"ROW_ID\")\n",
    "y_train = pd.read_csv(\"data/y_train.csv\", index_col=\"ROW_ID\")\n",
    "\n",
    "common_train_ids = X_train.index.intersection(y_train.index)\n",
    "X_train = X_train.loc[common_train_ids].copy()\n",
    "y_train = y_train.loc[common_train_ids].copy()\n",
    "assert X_train.index.equals(y_train.index), \"ROW_ID mismatch between X_train and y_train after alignment!\"\n",
    "\n",
    "# Binary sign target\n",
    "y = (y_train.iloc[:, 0] > 0).astype(int)\n",
    "\n",
    "# Groups by TS (kept ONLY for grouping / within-TS features inside fold)\n",
    "groups = X_train[\"TS\"].astype(str)\n",
    "\n",
    "# We keep raw columns for engineering (including TS/GROUP/turnover), but we will NOT pass TS/ALLOCATION through\n",
    "X_raw = X_train.copy()\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline: engineer -> preprocess -> RF\n",
    "# -------------------------\n",
    "feat_eng = TSFeatureEngineer(\n",
    "    ts_col=\"TS\",\n",
    "    group_col=\"GROUP\",\n",
    "    alloc_col=\"ALLOCATION\",\n",
    "    turnover_col=\"MEDIAN_DAILY_TURNOVER\",\n",
    "    ret_prefix=\"RET_\",\n",
    "    sv_prefix=\"SIGNED_VOLUME_\",\n",
    "    horizons=(3, 5, 10, 20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f274272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n",
      "cat: /proc/meminfo: No such file or directory\n",
      "cat: /etc/issue: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!cat /proc/meminfo | head -n 5\n",
    "!cat /etc/issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63772629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(527073, 44)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "392d29f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31870, 44)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88899507",
   "metadata": {},
   "source": [
    "# Random forest and LightGbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e85605cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority-class baseline accuracy: 0.507184\n"
     ]
    }
   ],
   "source": [
    "#BASELINE ACCURACY\n",
    "majority_acc = max(y.mean(), 1 - y.mean())\n",
    "print(f\"Majority-class baseline accuracy: {majority_acc:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d41d4fa",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab30802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\",\n",
    "         Pipeline(steps=[\n",
    "             # Tree models can't take NaNs in sklearn RF; keep imputation inside pipeline (no leakage).\n",
    "             (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "         ]),\n",
    "         selector(dtype_include=np.number)\n",
    "        ),\n",
    "        (\"cat\",\n",
    "         Pipeline(steps=[\n",
    "             (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "             (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "         ]),\n",
    "         selector(dtype_exclude=np.number)\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    n_jobs=6,\n",
    "    bootstrap=True,\n",
    "    max_samples=200_000,   # keep if you have enough rows; otherwise set None\n",
    "    max_depth=20,\n",
    "    min_samples_leaf=20,\n",
    "    max_features=\"sqrt\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"feats\", feat_eng),\n",
    "    (\"prep\", preprocess),\n",
    "    (\"rf\", rf),\n",
    "])\n",
    "\n",
    "# -------------------------\n",
    "# Leakage-safe GroupKFold on TS\n",
    "# -------------------------\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "scores = cross_val_score(\n",
    "    pipe,\n",
    "    X_raw,\n",
    "    y,\n",
    "    cv=gkf,\n",
    "    groups=groups,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "print(\"Random Forest GroupKFold-TS accuracy (with engineered features):\")\n",
    "print(\"folds:\", scores)\n",
    "print(\"mean :\", scores.mean())\n",
    "print(\"std  :\", scores.std(ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9dc36",
   "metadata": {},
   "source": [
    "# Now lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df1ee282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b992ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Preprocess\n",
    "# -------------------------\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\",\n",
    "         Pipeline(steps=[\n",
    "             (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "         ]),\n",
    "         selector(dtype_include=np.number)\n",
    "        ),\n",
    "        (\"cat\",\n",
    "         Pipeline(steps=[\n",
    "             (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "             (\"oh\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "         ]),\n",
    "         selector(dtype_exclude=np.number)\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1da9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_cv_accuracy_lgbm(\n",
    "    X_raw, y, groups,\n",
    "    feat_eng, preprocess,\n",
    "    lgbm_params,\n",
    "    n_splits=3,\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=False,\n",
    "    random_state=42\n",
    "):\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    fold_scores = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_raw, y, groups=groups), start=1):\n",
    "        X_tr_raw = X_raw.iloc[tr_idx]\n",
    "        y_tr = y.iloc[tr_idx]\n",
    "        X_va_raw = X_raw.iloc[va_idx]\n",
    "        y_va = y.iloc[va_idx]\n",
    "\n",
    "        # 1) Feature engineering (fit on train only)\n",
    "        fe = clone(feat_eng)\n",
    "        X_tr_fe = fe.fit_transform(X_tr_raw, y_tr)\n",
    "        X_va_fe = fe.transform(X_va_raw)\n",
    "\n",
    "        # 2) Preprocess (fit on train only)\n",
    "        pp = clone(preprocess)\n",
    "        X_tr_mat = pp.fit_transform(X_tr_fe, y_tr)\n",
    "        X_va_mat = pp.transform(X_va_fe)\n",
    "\n",
    "        # 3) Build identical feature names from fitted preprocess\n",
    "        feat_names = pp.get_feature_names_out()\n",
    "        if X_tr_mat.shape[1] != len(feat_names) or X_va_mat.shape[1] != len(feat_names):\n",
    "            raise ValueError(\n",
    "                f\"Feature mismatch: X_tr {X_tr_mat.shape[1]}, X_va {X_va_mat.shape[1]}, names {len(feat_names)}\"\n",
    "            )\n",
    "\n",
    "        # 4) Wrap matrices into DataFrames with identical columns (guaranteed alignment)\n",
    "        # Note: if matrices are sparse, pandas stores them as object; convert to dense if needed.\n",
    "        # For large data, it's usually better to keep sparse; instead, we can just satisfy the name check\n",
    "        # by setting feature_names_in_ via fitting on DataFrame.\n",
    "        X_tr = pd.DataFrame(X_tr_mat.toarray() if hasattr(X_tr_mat, \"toarray\") else X_tr_mat, columns=feat_names)\n",
    "        X_va = pd.DataFrame(X_va_mat.toarray() if hasattr(X_va_mat, \"toarray\") else X_va_mat, columns=feat_names)\n",
    "\n",
    "        model = LGBMClassifier(**lgbm_params, random_state=random_state, n_jobs=6)\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            eval_metric=\"binary_error\",\n",
    "            callbacks=[\n",
    "                early_stopping(stopping_rounds=early_stopping_rounds, verbose=False),\n",
    "                log_evaluation(period=50 if verbose_eval else 0),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_va)\n",
    "        acc = accuracy_score(y_va, y_pred)\n",
    "        fold_scores.append(acc)\n",
    "\n",
    "        if verbose_eval:\n",
    "            print(f\"Fold {fold}: acc={acc:.6f}, best_iter={getattr(model, 'best_iteration_', None)}\")\n",
    "\n",
    "    return np.array(fold_scores, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d5c753",
   "metadata": {},
   "source": [
    "# Coarse search (3-fold TS CV): num_leaves, min_data_in_leaf, max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74658b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Coarse screen TOP-2 (2-fold TS) ===\n",
      "\n",
      "#1 cfg={'num_leaves': 31, 'max_depth': 6, 'min_child_samples': 1000}\n",
      "folds: [0.52114856 0.52199735] mean: 0.5215729573354335 std: 0.0006001834658850511\n",
      "\n",
      "#2 cfg={'num_leaves': 63, 'max_depth': 8, 'min_child_samples': 50}\n",
      "folds: [0.52030247 0.5209082 ] mean: 0.5206053367516714 std: 0.00042831935747832654\n",
      "\n",
      "=== Confirmed best structure (3-fold TS) ===\n",
      "locked_structure: {'num_leaves': 31, 'max_depth': 6, 'min_child_samples': 1000}\n",
      "folds: [0.52167999 0.51710988 0.52336177] mean: 0.5207172117071036 std: 0.0032352360610674244\n",
      "\n",
      "Saved: structure_selection_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "base_params = dict(\n",
    "    objective=\"binary\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_estimators=2000,          # coarse/confirm budget\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    subsample_freq=1,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=0.0,\n",
    "    verbosity=-1,\n",
    "    force_row_wise=True,\n",
    ")\n",
    "\n",
    "coarse_structures = [\n",
    "    dict(num_leaves=63,  max_depth=8,  min_child_samples=50),\n",
    "    dict(num_leaves=127, max_depth=10, min_child_samples=50),\n",
    "    dict(num_leaves=255, max_depth=-1, min_child_samples=100),\n",
    "\n",
    "    dict(num_leaves=63,  max_depth=8,  min_child_samples=200),\n",
    "    dict(num_leaves=127, max_depth=10, min_child_samples=200),\n",
    "    dict(num_leaves=127, max_depth=-1, min_child_samples=300),\n",
    "    dict(num_leaves=255, max_depth=-1, min_child_samples=500),\n",
    "\n",
    "    dict(num_leaves=31,  max_depth=6,  min_child_samples=1000),\n",
    "    dict(num_leaves=31,  max_depth=6,  min_child_samples=2000),\n",
    "\n",
    "    dict(num_leaves=63,  max_depth=8,  min_child_samples=500),\n",
    "    dict(num_leaves=63,  max_depth=8,  min_child_samples=1000),\n",
    "\n",
    "    dict(num_leaves=127, max_depth=10, min_child_samples=500),\n",
    "    dict(num_leaves=127, max_depth=10, min_child_samples=1000),\n",
    "\n",
    "    dict(num_leaves=255, max_depth=-1, min_child_samples=1000),\n",
    "    dict(num_leaves=255, max_depth=-1, min_child_samples=2000),\n",
    "\n",
    "    dict(num_leaves=63,  max_depth=-1, min_child_samples=1000),\n",
    "    dict(num_leaves=127, max_depth=-1, min_child_samples=2000),\n",
    "]\n",
    "\n",
    "# ---- 1) Cheap coarse screen (2-fold TS) ----\n",
    "coarse_scores = []\n",
    "for cfg in coarse_structures:\n",
    "    params = {**base_params, **cfg}\n",
    "    scores = ts_cv_accuracy_lgbm(\n",
    "        X_raw=X_raw, y=y, groups=groups,\n",
    "        feat_eng=feat_eng, preprocess=preprocess,\n",
    "        lgbm_params=params,\n",
    "        n_splits=2,\n",
    "        early_stopping_rounds=150,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "    coarse_scores.append((float(scores.mean()), cfg, scores))\n",
    "\n",
    "coarse_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "top2_coarse = coarse_scores[:2]\n",
    "\n",
    "print(\"\\n=== Coarse screen TOP-2 (2-fold TS) ===\")\n",
    "for rank, (m, cfg, sc) in enumerate(top2_coarse, start=1):\n",
    "    print(f\"\\n#{rank} cfg={cfg}\")\n",
    "    print(\"folds:\", sc, \"mean:\", sc.mean(), \"std:\", sc.std(ddof=1))\n",
    "\n",
    "# ---- 2) Confirm top-2 with 3-fold TS ----\n",
    "confirmed = []\n",
    "for _, cfg, _ in top2_coarse:\n",
    "    params = {**base_params, **cfg}\n",
    "    scores = ts_cv_accuracy_lgbm(\n",
    "        X_raw=X_raw, y=y, groups=groups,\n",
    "        feat_eng=feat_eng, preprocess=preprocess,\n",
    "        lgbm_params=params,\n",
    "        n_splits=3,\n",
    "        early_stopping_rounds=200,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "    confirmed.append((float(scores.mean()), cfg, scores))\n",
    "\n",
    "confirmed.sort(key=lambda x: x[0], reverse=True)\n",
    "locked_structure = confirmed[0][1]\n",
    "\n",
    "print(\"\\n=== Confirmed best structure (3-fold TS) ===\")\n",
    "print(\"locked_structure:\", locked_structure)\n",
    "print(\"folds:\", confirmed[0][2], \"mean:\", confirmed[0][2].mean(), \"std:\", confirmed[0][2].std(ddof=1))\n",
    "\n",
    "# ---- 3) Save results (optional but recommended) ----\n",
    "payload = {\n",
    "    \"top2_coarse\": [\n",
    "        {\"rank\": i+1, \"mean\": m, \"cfg\": cfg, \"folds\": sc.tolist()}\n",
    "        for i, (m, cfg, sc) in enumerate(top2_coarse)\n",
    "    ],\n",
    "    \"top2_confirmed\": [\n",
    "        {\"rank\": i+1, \"mean\": m, \"cfg\": cfg, \"folds\": sc.tolist()}\n",
    "        for i, (m, cfg, sc) in enumerate(confirmed[:2])\n",
    "    ],\n",
    "    \"locked_structure\": locked_structure,\n",
    "}\n",
    "\n",
    "with open(\"structure_selection_results.json\", \"w\") as f:\n",
    "    json.dump(payload, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved: structure_selection_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1fb6ab",
   "metadata": {},
   "source": [
    "# Lock structure; fine-tune learning rate + regularization (3-fold TS CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a0f2985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fine-tune best (3-fold TS) ===\n",
      "locked_structure: {'num_leaves': 31, 'max_depth': 6, 'min_child_samples': 1000}\n",
      "best_fine_cfg  : {'colsample_bytree': 0.9, 'learning_rate': 0.03, 'reg_alpha': 0.0, 'reg_lambda': 8.0, 'subsample': 0.9}\n",
      "folds          : [0.52195888 0.51925003 0.52340162]\n",
      "mean           : 0.5215368440196885 std: 0.002107722458236562\n"
     ]
    }
   ],
   "source": [
    "locked_structure = confirmed[0][1]\n",
    "\n",
    "base_params[\"n_estimators\"] = 5000  #fine_tune budget\n",
    "fine_base_params = dict(\n",
    "    **base_params,\n",
    ")\n",
    "\n",
    "fine_grid = {\n",
    "    \"learning_rate\": [0.02, 0.03, 0.05],\n",
    "    \"subsample\": [0.8, 0.9],\n",
    "    \"colsample_bytree\": [0.8, 0.9],\n",
    "    \"reg_alpha\": [0.0, 0.5, 1.0],\n",
    "    \"reg_lambda\": [0.0, 1.0, 2.0, 4.0, 8.0],\n",
    "}\n",
    "\n",
    "best2 = None\n",
    "best2_mean = -np.inf\n",
    "\n",
    "for cfg in ParameterGrid(fine_grid):\n",
    "    params = {**fine_base_params, **locked_structure, **cfg}\n",
    "    scores = ts_cv_accuracy_lgbm(\n",
    "        X_raw=X_raw, y=y, groups=groups,\n",
    "        feat_eng=feat_eng, preprocess=preprocess,\n",
    "        lgbm_params=params,\n",
    "        n_splits=3,\n",
    "        early_stopping_rounds=200,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "    mean_acc = float(scores.mean())\n",
    "    if mean_acc > best2_mean:\n",
    "        best2_mean = mean_acc\n",
    "        best2 = (cfg, scores)\n",
    "    print(mean_acc)\n",
    "\n",
    "best_params = {**fine_base_params, **locked_structure, **best2[0]}\n",
    "\n",
    "print(\"\\n=== Fine-tune best (3-fold TS) ===\")\n",
    "print(\"locked_structure:\", locked_structure)\n",
    "print(\"best_fine_cfg  :\", best2[0])\n",
    "print(\"folds          :\", best2[1])\n",
    "print(\"mean           :\", best2[1].mean(), \"std:\", best2[1].std(ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b49cf",
   "metadata": {},
   "source": [
    "# Final evaluation: 5-fold TS CV with the tuned params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8db522c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL 5-fold TS CV (LightGBM tuned) ===\n",
      "folds: [0.51780632 0.52572762 0.51934771 0.52607714 0.52287086]\n",
      "mean : 0.5223659298074346\n",
      "std  : 0.0037158003627439052\n",
      "\n",
      "Best LightGBM params used:\n",
      "{'objective': 'binary', 'boosting_type': 'gbdt', 'n_estimators': 5000, 'learning_rate': 0.03, 'subsample': 0.9, 'subsample_freq': 1, 'colsample_bytree': 0.9, 'reg_alpha': 0.0, 'reg_lambda': 8.0, 'verbosity': -1, 'force_row_wise': True, 'num_leaves': 31, 'max_depth': 6, 'min_child_samples': 1000}\n"
     ]
    }
   ],
   "source": [
    "final_scores = ts_cv_accuracy_lgbm(\n",
    "    X_raw=X_raw, y=y, groups=groups,\n",
    "    feat_eng=feat_eng, preprocess=preprocess,\n",
    "    lgbm_params=best_params,\n",
    "    n_splits=5,\n",
    "    early_stopping_rounds=200,\n",
    "    verbose_eval=False,\n",
    ")\n",
    "\n",
    "print(\"\\n=== FINAL 5-fold TS CV (LightGBM tuned) ===\")\n",
    "print(\"folds:\", final_scores)\n",
    "print(\"mean :\", final_scores.mean())\n",
    "print(\"std  :\", final_scores.std(ddof=1))\n",
    "\n",
    "print(\"\\nBest LightGBM params used:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70d41dd",
   "metadata": {},
   "source": [
    "# predictions with model of my choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334b9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        pred\n",
      "ROW_ID      \n",
      "527073     1\n",
      "527074     0\n",
      "527075     0\n",
      "527076     1\n",
      "527077     0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "# -------------------------\n",
    "# 1) Fit FE on full train, transform train/test\n",
    "# -------------------------\n",
    "fe_final = clone(feat_eng)\n",
    "X_tr_fe = fe_final.fit_transform(X_raw, y)     # fit only on train\n",
    "X_te_fe = fe_final.transform(X_test)           # transform test\n",
    "\n",
    "# -------------------------\n",
    "# 2) Fit preprocess on full train, transform train/test\n",
    "# -------------------------\n",
    "pp_final = clone(preprocess)\n",
    "X_tr_mat = pp_final.fit_transform(X_tr_fe, y)\n",
    "X_te_mat = pp_final.transform(X_te_fe)\n",
    "\n",
    "feat_names = pp_final.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrames with consistent columns\n",
    "X_tr = pd.DataFrame(\n",
    "    X_tr_mat.toarray() if hasattr(X_tr_mat, \"toarray\") else X_tr_mat,\n",
    "    columns=feat_names,\n",
    "    index=X_raw.index\n",
    ")\n",
    "X_te = pd.DataFrame(\n",
    "    X_te_mat.toarray() if hasattr(X_te_mat, \"toarray\") else X_te_mat,\n",
    "    columns=feat_names,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 3) Train final model on full training set\n",
    "# -------------------------\n",
    "final_model = LGBMClassifier(**best_params, random_state=42, n_jobs=6)\n",
    "\n",
    "final_model.fit(X_tr, y)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Predict on X_test\n",
    "# -------------------------\n",
    "proba_test = final_model.predict_proba(X_te)[:, 1]\n",
    "pred_test  = (proba_test >= 0.5).astype(int)   # or use final_model.predict(X_te)\n",
    "\n",
    "# Save with ROW_ID index\n",
    "pred_df = pd.DataFrame(\n",
    "    {\"pred\": pred_test},\n",
    "    index=X_test.index\n",
    ")\n",
    "pred_df.index.name = \"ROW_ID\"\n",
    "\n",
    "os.makedirs(\"submission\", exist_ok=True)\n",
    "pred_df.to_csv(\"submission/lgbm_test_predictions.csv\")\n",
    "print(pred_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84ef10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
